\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{listings}
\usepackage{xcolor}

\geometry{a4paper, margin=1in}

\definecolor{codebg}{rgb}{0.95,0.95,0.95}
\lstset{
    backgroundcolor=\color{codebg},
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    numbersep=5pt,
    tabsize=4,
    language=C,
    showspaces=false,
    showstringspaces=false,
    breaklines=true,
    captionpos=b
}

\title{SOFTWARE ASSIGNMENT}
\author{EE24BTECH11004 - ANKIT}

\begin{document}
\maketitle

\section{What Are Eigenvalues?}

Eigenvalues are a fundamental concept in linear algebra, widely used in mathematics, physics, engineering, and computer science. They describe important properties of matrices and linear transformations.

\subsection{Formal Definition}

Given a square matrix $A$ of size $n \times n$, a scalar $\lambda$ is called an \textbf{eigenvalue} of $A$ if there exists a non-zero vector $\mathbf{v}$ (called an \textbf{eigenvector}) such that:
$A \mathbf{v} = \lambda \mathbf{v}.$

Here:
\begin{itemize}
    \item $A$: The matrix representing the linear transformation.
    \item $\mathbf{v}$: The eigenvector associated with $\lambda$.
    \item $\lambda$: The eigenvalue.
\end{itemize}

\subsection{Key Points}
\begin{itemize}
    \item \textbf{Geometric Interpretation:}
    Eigenvalues represent \textit{scaling factors} by which eigenvectors are stretched or compressed during the linear transformation defined by $A$. Eigenvectors remain in the same or opposite direction after the transformation.
    
    \item \textbf{Algebraic Interpretation:}
    To find eigenvalues, we solve the \textit{characteristic equation}:
    $\det(A - \lambda I) = 0,$
    where $I$ is the identity matrix of the same size as $A$.
\end{itemize}

\subsection{Examples}
\begin{enumerate}
    \item \textbf{Diagonal Matrix:} \\
    If $A = \begin{bmatrix} 3 & 0 \\ 0 & 5 \end{bmatrix}$, the eigenvalues are the diagonal elements:
    $\lambda_1 = 3, \quad \lambda_2 = 5.$
    
    \item \textbf{Non-Diagonal Matrix:} \\
    For $A = \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}$, the eigenvalues are obtained by solving:
    $\det\left(\begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix} - \lambda \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}\right) = 0.$
    The eigenvalues are:
    $\lambda_1 = 3, \quad \lambda_2 = -1.$
\end{enumerate}

\subsection{Properties of Eigenvalues}
\begin{itemize}
    \item \textbf{Trace and Determinant:}
    \begin{itemize}
        \item The sum of the eigenvalues equals the \textbf{trace} of the matrix: $\text{tr}(A) = \sum \lambda_i.$
        \item The product of the eigenvalues equals the \textbf{determinant} of the matrix: $\det(A) = \prod \lambda_i.$
    \end{itemize}
    
    \item \textbf{Multiplicity:}
    \begin{itemize}
        \item \textit{Algebraic Multiplicity:} The number of times an eigenvalue $\lambda$ appears as a root of the characteristic polynomial.
        \item \textit{Geometric Multiplicity:} The dimension of the eigenspace corresponding to $\lambda$ (the set of all eigenvectors associated with $\lambda$).
    \end{itemize}
    
    \item \textbf{Symmetric Matrices:}
    \begin{itemize}
        \item All eigenvalues of symmetric matrices are real.
        \item Eigenvectors corresponding to different eigenvalues are orthogonal.
    \end{itemize}
    
    \item \textbf{Stability Analysis:}
    Eigenvalues determine the stability of dynamical systems:
    \begin{itemize}
        \item If all eigenvalues have negative real parts, the system is stable.
        \item Positive real parts indicate instability.
    \end{itemize}
\end{itemize}

\subsection{Conclusion}
Eigenvalues provide critical insight into the behavior of a matrix and the systems it represents. They are indispensable in solving practical problems across scientific and engineering domains.
\section{Chosen Algorithm: Jacobi Method}
The Jacobi method is an iterative algorithm primarily used for computing the eigenvalues of real symmetric matrices. It transforms the matrix into a diagonal form through successive orthogonal similarity transformations, where the diagonal elements eventually approximate the eigenvalues.

\subsection{Description of the Jacobi Method}
Given a symmetric matrix $A$, the method aims to zero out the largest off-diagonal elements through a series of rotations. Each rotation targets a specific pair of indices $(p, q)$ to reduce the value of $A_{pq}$ to zero, thereby moving the matrix closer to a diagonal form.

\subsection{Algorithm Steps}
\begin{enumerate}
    \item Find the largest off-diagonal element $a_{pq}$.
    \item Compute the rotation angle $\theta$ to zero out $a_{pq}$ using:
    \[
    \theta = \frac{1}{2} \tan^{-1} \left( \frac{2a_{pq}}{a_{pp} - a_{qq}} \right)
    \]
    \item Construct the rotation matrix $P$ and apply the transformation $A' = P^T A P$.
    \item Repeat until all off-diagonal elements are below a given tolerance.
\end{enumerate}

\section{Time Complexity Analysis}
The Jacobi method has a per-iteration complexity of $O(n^3)$ for an $n \times n$ matrix due to matrix multiplication and rotation operations. The total complexity depends on the number of iterations required for convergence, typically making it less suitable for very large matrices compared to methods such as the QR algorithm.

\section{Other Insights}
\subsection{Memory Usage}
The method requires $O(n^2)$ space for storing matrix elements, making it feasible for small to moderately sized matrices.

\subsection{Convergence Rate}
The Jacobi method exhibits quadratic convergence for symmetric matrices. Although slower than other methods such as the QR algorithm, it can achieve high accuracy for all eigenvalues when convergence is reached.

\subsection{Suitability}
The Jacobi method is best suited for small to medium-sized symmetric matrices, where its simplicity and accuracy are advantageous. It is less efficient for large matrices or non-symmetric matrices, where other methods, like the QR algorithm, perform better.

\section{Comparison of Algorithms}
\begin{itemize}
    \item \textbf{Jacobi Method}: Complexity of $O(n^3)$ per iteration; accurate and straightforward for symmetric matrices; slower convergence but robust for smaller problems.
    \item \textbf{QR Algorithm}: Generally faster convergence for large matrices; handles both symmetric and non-symmetric matrices; $O(n^3)$ complexity per iteration.
    \item \textbf{Power Iteration}: Efficient for finding the dominant eigenvalue; $O(n^2)$ per iteration; limited to finding one eigenvalue at a time.
    \item \textbf{Divide-and-Conquer Methods}: Faster for large matrices but more complex to implement; used in advanced libraries for dense eigenproblems.
\end{itemize}

\section{Conclusion}
The Jacobi method is a robust approach for computing the eigenvalues of symmetric matrices. While it may be slower and less efficient for large-scale problems, it offers simplicity and high accuracy in cases where it is applicable.

\section{Implementation in C}
Below is the library used for implementing code
\lstinputlisting[language=C]{lib.c}

Below is a detailed explanation of the QR algorithm implementation in C.

\subsubsection{QR Decomposition Function}
The \texttt{qr\_decomposition()} function performs the QR decomposition of matrix $A$ into matrices $Q$ and $R$ using the Modified Gram-Schmidt process. It ensures $Q$ is orthogonal, and $R$ is upper triangular.

\lstinputlisting[language=C]{qrdecomposition.c}

\begin{itemize}
    \item \textbf{Orthogonalization:} Each column of $A$ is iteratively orthogonalized with respect to the previous columns.  
    $R[i][j] = Q[:, i]^T \cdot A[:, j]$  
    $Q[:, j] = A[:, j] - \sum_{i=0}^{j-1} R[i][j] \cdot Q[:, i]$
    \item \textbf{Normalization:} After orthogonalization, each column vector of $Q$ is normalized:  
    $Q[:, j] = \frac{Q[:, j]}{\|Q[:, j]\|}$
\end{itemize}

\subsubsection{Matrix Multiplication Function}
The \texttt{multiply\_matrices()} function computes the product $R \cdot Q$, updating the matrix $A$ in each iteration.

\lstinputlisting[language=C]{matrixmultiplication.c}

\begin{itemize}
    \item This step recomposes the matrix $A$ using:  
    $A' = R \cdot Q$  
    where $R$ is upper triangular, and $Q$ is orthogonal from the previous decomposition.
    \item This operation ensures that the matrix evolves toward a diagonal form as the iterations proceed.
\end{itemize}

\subsubsection{Diagonal Check Function}
The \texttt{is\_diagonal()} function verifies if $A$ is sufficiently close to a diagonal matrix by comparing the magnitude of off-diagonal elements to a tolerance value (\texttt{TOL}).

\lstinputlisting[language=C]{diagonalcheck.c}

\begin{itemize}
    \item \textbf{Tolerance Comparison:} If all off-diagonal elements satisfy:  
    $|A[i][j]| < \text{TOL}, \quad \forall \, i \neq j$  
    the matrix is considered diagonal.
    \item \textbf{Purpose:} Ensures convergence by monitoring the evolution of $A$ during iterations.
\end{itemize}

\subsubsection{Main Iterative Loop}
The iterative loop orchestrates the steps of the QR algorithm until $A$ converges to a diagonal matrix or the maximum number of iterations (\texttt{MAX\_ITER}) is reached.

\lstinputlisting[language=C]{main.c}

\begin{itemize}
    \item \textbf{Step 1:} Perform QR decomposition of $A$ into $Q$ and $R$.
    \item \textbf{Step 2:} Update $A$ using the matrix product $R \cdot Q$.
    \item \textbf{Step 3:} Check convergence using \texttt{is\_diagonal()}.
    \item \textbf{Step 4:} Extract eigenvalues from the diagonal elements of $A$ if convergence is achieved:  
    $\text{Eigenvalues: } \lambda_i = A[i][i], \quad i = 1, 2, \ldots, n$
\end{itemize}

\section*{Time Complexity Analysis}

The given program implements the QR algorithm to compute the eigenvalues of a square matrix of size $n \times n$. The time complexity can be summarized as follows:

\begin{itemize}
    \item \textbf{QR Decomposition:} $O(n^3)$  
    This step involves orthogonalization and normalization, both contributing to $O(n^3)$.

    \item \textbf{Matrix Multiplication:} $O(n^3)$  
    Multiplying two $n \times n$ matrices takes $O(n^3)$.

    \item \textbf{Diagonal Check:} $O(n^2)$  
    Checking if the matrix is diagonal requires examining $n^2$ elements.

    \item \textbf{Overall Per Iteration:} $O(n^3)$  
    The dominant operations in each iteration are QR decomposition and matrix multiplication.

    \item \textbf{Total Complexity:} $O(k \cdot n^3)$  
    where $k$ is the number of iterations required for convergence. In the worst case, $k = \text{MAX\_ITER}$, making the complexity:  
    $O(\text{MAX\_ITER} \cdot n^3)$  
    If \texttt{MAX\_ITER} is treated as a constant, the complexity simplifies to $O(n^3)$ for practical purposes.
\end{itemize}

\section*{Efficiency Analysis of the QR Algorithm}

\subsection*{Strengths}
\begin{itemize}
    \item \textbf{Accuracy:} The QR algorithm is numerically stable and provides precise eigenvalues, particularly for symmetric or Hermitian matrices.
    \item \textbf{Versatility:} It can handle both real and complex eigenvalues effectively.
    \item \textbf{Reliability:} The algorithm is robust and well-studied for most types of square matrices.
\end{itemize}

\subsection*{Weaknesses}
\begin{itemize}
    \item \textbf{Computational Cost:} 
    The time complexity is $O(k \cdot n^3)$, where $k$ is the number of iterations required for convergence. This can be expensive for large matrices.
    \item \textbf{Scalability:} The algorithm is inefficient for very large matrices, where iterative methods such as Lanczos or Arnoldi are more suitable.
\end{itemize}

\subsection*{Practical Use}
\begin{itemize}
    \item \textbf{Small Matrices:} Efficient and accurate for matrices with $n < 1000$.
    \item \textbf{Large Matrices:} Computationally expensive, making iterative methods a better choice for high-dimensional problems.
\end{itemize}

\subsection{Output of the Program}
When the above code is run, the following results are printed:
\begin{itemize}
    \item The original matrix $A$.
    \item The computed eigenvalues.
\end{itemize}
\lstinputlisting[language={[LaTeX]TeX}, caption={input}]{input.tex}
\lstinputlisting[language={[LaTeX]TeX}, caption={output}]{output.tex}


\end{document}
